{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0tPMJeBb5gN"
      },
      "source": [
        "# Train an Object Detection Model on Pascal VOC 2007 using KerasCV\n",
        "\n",
        "**Author:** [lukewood](https://twitter.com/luke_wood_ml)<br>\n",
        "**Date created:** 2022/08/22<br>\n",
        "**Last modified:** 2022/08/22<br>\n",
        "**Description:** Use KerasCV to train a RetinaNet on Pascal VOC 2007."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuiKt6D4b5gO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "KerasCV offers a complete set of APIs to train your own state-of-the-art,\n",
        "production-grade object detection model.  These APIs include object detection specific\n",
        "data augmentation techniques, and batteries included object detection models.\n",
        "\n",
        "To get started, let's sort out all of our imports and define global configuration parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9I2Ci9tb5gO"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "import keras_cv\n",
        "from keras_cv import bounding_box\n",
        "import os\n",
        "import resource\n",
        "from luketils import visualization\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = int(os.getenv(\"EPOCHS\", \"1\"))\n",
        "# To fully train a RetinaNet, comment out this line.\n",
        "# EPOCHS = 100\n",
        "CHECKPOINT_PATH = os.getenv(\"CHECKPOINT_PATH\", \"checkpoint/\")\n",
        "INFERENCE_CHECKPOINT_PATH = os.getenv(\"INFERENCE_CHECKPOINT_PATH\", CHECKPOINT_PATH)\n",
        "\n",
        "low, high = resource.getrlimit(resource.RLIMIT_NOFILE)\n",
        "resource.setrlimit(resource.RLIMIT_NOFILE, (high, high))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2b4qAnqb5gP"
      },
      "source": [
        "## Data loading\n",
        "\n",
        "KerasCV has a predefined specificication for bounding boxes.  To comply with this, you\n",
        "should package your bounding boxes into a dictionary matching the speciciation below:\n",
        "\n",
        "```\n",
        "bounding_boxes = {\n",
        "    # num_boxes may be a Ragged dimension\n",
        "    'boxes': Tensor(shape=[batch, num_boxes, 4]),\n",
        "    'classes': Tensor(shape=[batch, num_boxes])\n",
        "}\n",
        "```\n",
        "\n",
        "\n",
        "`bounding_boxes['boxes']` contains the coordinates of your bounding box in a KerasCV\n",
        "supported `bounding_box_format`.\n",
        "KerasCV requires a `bounding_box_format` argument in all components that process\n",
        "bounding boxes.\n",
        "This is done to maximize users' ability to plug and play individual components into their\n",
        "object detection components.\n",
        "\n",
        "To match the KerasCV API style, it is recommended that when writing a\n",
        "custom data loader, you also support a `bounding_box_format` argument.\n",
        "This makes it clear to those invoking your data loader what format the bounding boxes\n",
        "are in.\n",
        "\n",
        "For example:\n",
        "\n",
        "```python\n",
        "train_ds, ds_info = your_data_loader.load(\n",
        "    split='train', bounding_box_format='xywh', batch_size=8\n",
        ")\n",
        "```\n",
        "\n",
        "Clearly yields bounding boxes in the format `xywh`.  You can read more about\n",
        "KerasCV bounding box formats [in the API docs](https://keras.io/api/keras_cv/bounding_box/formats/).\n",
        "\n",
        "Our data comesloaded into the format\n",
        "`{\"images\": images, \"bounding_boxes\": bounding_boxes}`.  This format is supported in all\n",
        "KerasCV preprocessing components.\n",
        "\n",
        "Let's load some data and verify that our data looks as we expect it to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drSOH1QIb5gQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "def unpackage_tfds_inputs(inputs):\n",
        "    image = inputs[\"image\"]\n",
        "    boxes = keras_cv.bounding_box.convert_format(\n",
        "        inputs[\"objects\"][\"bbox\"],\n",
        "        images=image,\n",
        "        source=\"rel_yxyx\",\n",
        "        target=\"xywh\",\n",
        "    )\n",
        "    bounding_boxes = {\n",
        "        \"classes\": tf.cast(inputs[\"objects\"][\"label\"], dtype=tf.float32),\n",
        "        \"boxes\": tf.cast(boxes, dtype=tf.float32),\n",
        "    }\n",
        "    return {\"images\": tf.cast(image, tf.float32), \"bounding_boxes\": bounding_boxes}\n",
        "\n",
        "\n",
        "train_ds = tfds.load(\n",
        "    \"voc/2007\", split=\"train+validation\", with_info=False, shuffle_files=True\n",
        ")\n",
        "# add pascal 2012 dataset to augment the training set\n",
        "train_ds = train_ds.concatenate(\n",
        "    tfds.load(\"voc/2012\", split=\"train+validation\", with_info=False, shuffle_files=True)\n",
        ")\n",
        "eval_ds = tfds.load(\"voc/2007\", split=\"test\", with_info=False)\n",
        "\n",
        "train_ds = train_ds.map(unpackage_tfds_inputs, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "eval_ds = eval_ds.map(unpackage_tfds_inputs, num_parallel_calls=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vC93Ozn-b5gQ"
      },
      "source": [
        "Next, lets batch our data.  In KerasCV object detection tasks it is recommended that\n",
        "users use ragged batches.  This is due to the fact that images may be of different\n",
        "sizes in PascalVOC and that there may be different numbers of bounding boxes per image.\n",
        "\n",
        "The easiest way to construct a ragged dataset in a `tf.data` pipeline is to use\n",
        "`tf.data.experimental.dense_to_ragged_batch`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWUCGIc0b5gQ"
      },
      "outputs": [],
      "source": [
        "train_ds = train_ds.apply(tf.data.experimental.dense_to_ragged_batch(BATCH_SIZE))\n",
        "eval_ds = eval_ds.apply(tf.data.experimental.dense_to_ragged_batch(BATCH_SIZE))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-AOo_Vlb5gQ"
      },
      "source": [
        "Let's make sure our datasets look as we expect them to:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9HmklyIb5gR"
      },
      "outputs": [],
      "source": [
        "class_ids = [\n",
        "    \"Aeroplane\",\n",
        "    \"Bicycle\",\n",
        "    \"Bird\",\n",
        "    \"Boat\",\n",
        "    \"Bottle\",\n",
        "    \"Bus\",\n",
        "    \"Car\",\n",
        "    \"Cat\",\n",
        "    \"Chair\",\n",
        "    \"Cow\",\n",
        "    \"Dining Table\",\n",
        "    \"Dog\",\n",
        "    \"Horse\",\n",
        "    \"Motorbike\",\n",
        "    \"Person\",\n",
        "    \"Potted Plant\",\n",
        "    \"Sheep\",\n",
        "    \"Sofa\",\n",
        "    \"Train\",\n",
        "    \"Tvmonitor\",\n",
        "    \"Total\",\n",
        "]\n",
        "class_mapping = dict(zip(range(len(class_ids)), class_ids))\n",
        "\n",
        "\n",
        "def visualize_dataset(dataset, bounding_box_format):\n",
        "    sample = next(iter(dataset))\n",
        "    images, boxes = sample[\"images\"], sample[\"bounding_boxes\"]\n",
        "    visualization.plot_bounding_box_gallery(\n",
        "        images,\n",
        "        value_range=(0, 255),\n",
        "        bounding_box_format=bounding_box_format,\n",
        "        y_true=boxes,\n",
        "        scale=4,\n",
        "        rows=2,\n",
        "        cols=2,\n",
        "        show=True,\n",
        "        thickness=4,\n",
        "        font_scale=1,\n",
        "        class_mapping=class_mapping,\n",
        "    )\n",
        "\n",
        "\n",
        "visualize_dataset(train_ds, bounding_box_format=\"xywh\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mJJrBaEb5gR"
      },
      "source": [
        "and our eval set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UB1Mx_Bab5gR"
      },
      "outputs": [],
      "source": [
        "visualize_dataset(eval_ds, bounding_box_format=\"xywh\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCEQWTUZb5gR"
      },
      "source": [
        "Looks like everything is structured as expected.  Now we can move on to constructing our\n",
        "data augmentation pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4CSqGCsb5gR"
      },
      "source": [
        "## Data augmentation\n",
        "\n",
        "One of the most labor-intensive tasks when constructing object detection pipelines is\n",
        "data augmentation.  Image augmentation techniques must be aware of the underlying\n",
        "bounding boxes, and must update them accordingly.\n",
        "\n",
        "Luckily, KerasCV natively supports bounding box augmentation with its extensive library\n",
        "of [data augmentation layers](https://keras.io/api/keras_cv/layers/preprocessing/).\n",
        "The code below loads the Pascal VOC dataset, and performs on-the-fly bounding box\n",
        "friendly data augmentation inside of a `tf.data` pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7IEHvHZb5gR"
      },
      "outputs": [],
      "source": [
        "augment = keras_cv.layers.Augmenter(\n",
        "    layers=[\n",
        "        keras_cv.layers.RandomFlip(mode=\"horizontal\", bounding_box_format=\"xywh\"),\n",
        "        keras_cv.layers.RandAugment(\n",
        "            value_range=(0, 255),\n",
        "            rate=0.5,\n",
        "            magnitude=0.25,\n",
        "            augmentations_per_image=2,\n",
        "            geometric=False,\n",
        "        ),\n",
        "        keras_cv.layers.JitteredResize(\n",
        "            target_size=(640, 640), scale_factor=(0.75, 1.3), bounding_box_format=\"xywh\"\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "train_ds = train_ds.map(\n",
        "    lambda x: augment(x, training=True), num_parallel_calls=tf.data.AUTOTUNE\n",
        ")\n",
        "visualize_dataset(train_ds, bounding_box_format=\"xywh\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjNhqSD8b5gR"
      },
      "source": [
        "Great!  We now have a bounding box friendly augmentation pipeline.\n",
        "\n",
        "Next, let's construct our eval pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEI5ISk1b5gS"
      },
      "outputs": [],
      "source": [
        "inference_resizing = keras_cv.layers.Resizing(\n",
        "    640, 640, bounding_box_format=\"xywh\", pad_to_aspect_ratio=True\n",
        ")\n",
        "eval_ds = eval_ds.map(inference_resizing, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "visualize_dataset(eval_ds, bounding_box_format=\"xywh\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b76K7KpHb5gS"
      },
      "source": [
        "Finally, let's unpackage our inputs from the preprocessing dictionary, and prepare to feed\n",
        "the inputs into our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-YSn4fDb5gS"
      },
      "outputs": [],
      "source": [
        "\n",
        "def dict_to_tuple(inputs):\n",
        "    return inputs[\"images\"], bounding_box.to_dense(\n",
        "        inputs[\"bounding_boxes\"], max_boxes=32\n",
        "    )\n",
        "\n",
        "\n",
        "train_ds = train_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "eval_ds = eval_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
        "eval_ds = eval_ds.prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdViwHvAb5gS"
      },
      "source": [
        "Our data pipeline is now complete.  We can now move on to model creation and training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMRDc5xub5gS"
      },
      "source": [
        "## Model creation\n",
        "\n",
        "We'll use the KerasCV API to construct a RetinaNet model.  In this tutorial we use\n",
        "a pretrained ResNet50 backbone, initializing the weights to weights produced by training\n",
        "on the imagenet dataset.  In order to perform fine-tuning, we\n",
        "freeze the backbone before training.  When `include_rescaling=True` is set, inputs to\n",
        "the model are expected to be in the range `[0, 255]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjLn-iuqb5gS"
      },
      "outputs": [],
      "source": [
        "model = keras_cv.models.RetinaNet(\n",
        "    # number of classes to be used in box classification\n",
        "    classes=20,\n",
        "    # For more info on supported bounding box formats, visit\n",
        "    # https://keras.io/api/keras_cv/bounding_box/\n",
        "    bounding_box_format=\"xywh\",\n",
        "    # KerasCV offers a set of pre-configured backbones\n",
        "    backbone=keras_cv.models.ResNet50(\n",
        "        include_top=False, weights=\"imagenet\", include_rescaling=True\n",
        "    ).as_backbone(),\n",
        ")\n",
        "# For faster convergence, freeze the feature extraction filters by setting:\n",
        "model.backbone.trainable = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqS8Mc6Wb5gS"
      },
      "source": [
        "That is all it takes to construct a KerasCV RetinaNet.  The RetinaNet accepts tuples of\n",
        "dense image Tensors and bounding box dictionaries to `fit()` and `train_on_batch()`\n",
        "This matches what we have constructed in our input pipeline above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJ1gw-5rb5gS"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.TensorBoard(log_dir=\"logs\"),\n",
        "    keras.callbacks.ModelCheckpoint(CHECKPOINT_PATH, save_weights_only=True),\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okSQPPs1b5gS"
      },
      "source": [
        "## Training our model\n",
        "\n",
        "All that is left to do is train our model.  KerasCV object detection models follow the\n",
        "standard Keras workflow, leveraging `compile()` and `fit()`.\n",
        "\n",
        "Let's compile our model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQCVzWURb5gS"
      },
      "outputs": [],
      "source": [
        "# including a global_clipnorm is extremely important in object detection tasks\n",
        "base_lr = 0.01\n",
        "lr_decay = tf.keras.optimizers.schedules.PiecewiseConstantDecay(\n",
        "    boundaries=[12000 * 16, 16000 * 16],\n",
        "    values=[base_lr, 0.1 * base_lr, 0.01 * base_lr],\n",
        ")\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(\n",
        "    learning_rate=lr_decay, momentum=0.9, global_clipnorm=10.0\n",
        ")\n",
        "model.compile(\n",
        "    classification_loss=\"focal\",\n",
        "    box_loss=\"smoothl1\",\n",
        "    optimizer=optimizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGzjZ926b5gS"
      },
      "source": [
        "And run `model.fit()`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ka9DL16ib5gS"
      },
      "outputs": [],
      "source": [
        "model.fit(\n",
        "    train_ds,\n",
        "    validation_data=eval_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=callbacks,\n",
        ")\n",
        "# you can also save model weights with: `model.save_weights(CHECKPOINT_PATH)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnPJKGhDb5gS"
      },
      "source": [
        "## Inference\n",
        "\n",
        "KerasCV makes object detection inference simple.  `model.predict(images)` returns a\n",
        "RaggedTensor of bounding boxes.  By default, `RetinaNet.predict()` will perform\n",
        "a non max suppression operation for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnEoPezdb5gT"
      },
      "outputs": [],
      "source": [
        "model.load_weights(INFERENCE_CHECKPOINT_PATH)\n",
        "\n",
        "\n",
        "def visualize_detections(model, bounding_box_format):\n",
        "    images, y_true = next(iter(eval_ds.take(1)))\n",
        "    y_pred = model.predict(images)\n",
        "    y_pred = bounding_box.to_ragged(y_pred)\n",
        "    visualization.plot_bounding_box_gallery(\n",
        "        images,\n",
        "        value_range=(0, 255),\n",
        "        bounding_box_format=bounding_box_format,\n",
        "        y_true=y_true,\n",
        "        y_pred=y_pred,\n",
        "        scale=4,\n",
        "        rows=3,\n",
        "        cols=3,\n",
        "        show=True,\n",
        "        thickness=4,\n",
        "        font_scale=1,\n",
        "        class_mapping=class_mapping,\n",
        "    )\n",
        "\n",
        "\n",
        "visualize_detections(model, bounding_box_format=\"xywh\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xpsul8xtb5gT"
      },
      "source": [
        "To get good results, you should train for at least 50~ epochs.  You also may need to\n",
        "tune the prediction decoder layer.  This can be done by passing a custom prediction\n",
        "decoder to the RetinaNet constructor as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHwhFGT6b5gT"
      },
      "outputs": [],
      "source": [
        "prediction_decoder = keras_cv.layers.MultiClassNonMaxSuppression(\n",
        "    bounding_box_format=\"xywh\",\n",
        "    from_logits=True,\n",
        "    # Decrease the required threshold to make predictions get pruned out\n",
        "    iou_threshold=0.35,\n",
        "    # Tune confidence threshold for predictions to pass NMS\n",
        "    confidence_threshold=0.95,\n",
        ")\n",
        "model.prediction_decoder = prediction_decoder\n",
        "visualize_detections(model, bounding_box_format=\"xywh\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enE32JjRb5gT"
      },
      "source": [
        "## Results and conclusions\n",
        "\n",
        "KerasCV makes it easy to construct state-of-the-art object detection pipelines.  All of\n",
        "the KerasCV object detection components can be used independently, but also have deep\n",
        "integration with each other.  With KerasCV, bounding box augmentation and more,\n",
        "are all made simple and consistent.\n",
        "\n",
        "Some follow up exercises for the reader:\n",
        "\n",
        "- add additional augmentation techniques to improve model performance\n",
        "- grid search `confidence_threshold` and `iou_threshold` on `MultiClassNonMaxSuppression` to\n",
        "    achieve an optimal Mean Average Precision\n",
        "- tune the hyperparameters and data augmentation used to produce high quality results\n",
        "- train an object detection model on another dataset"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "retina_net_overview",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification\n",
    "\n",
    "The task of image classification for CIFAR-10 is to correctly classify images of 10 different classes into their respective categories. \n",
    "\n",
    "The dataset contains 50,000 training images and 10,000 test images, each of size 32x32 pixels and color channels in RGB format. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the CIFAR-10 Dataset\n",
    "\n",
    "The CIFAR-10 dataset is a widely used benchmark dataset for image classification tasks. It consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. The 10 classes are:\n",
    "\n",
    "Airplane\n",
    "Automobile\n",
    "Bird\n",
    "Cat\n",
    "Deer\n",
    "Dog\n",
    "Frog\n",
    "Horse\n",
    "Ship\n",
    "Truck\n",
    "The dataset is divided into training and testing sets, with 50,000 images for training and 10,000 images for testing. The images in the dataset are relatively small and low-resolution compared to other image classification datasets, making it a challenging benchmark for evaluating image classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "original_y_test = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets explore the dataset. It has 10 different classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the class names\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x_train[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(class_names[y_train[i][0]])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x_test[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(class_names[y_test[i][0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets scale the dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "This is a convolutional neural network (CNN) architecture with three convolutional layers followed by two dense (fully connected) layers. The input shape for this model is (32, 32, 3), which means it takes a 32x32 RGB image as input.\n",
    "\n",
    "The first layer in the network is a 2D convolutional layer with 32 filters of size 3x3, followed by a max pooling layer with a pool size of 2x2. The second and third layers are also 2D convolutional layers with 64 filters of size 3x3, each followed by another max pooling layer with a pool size of 2x2.\n",
    "\n",
    "The output of the third convolutional layer is then flattened and passed through two dense layers. The first dense layer has 64 neurons with a ReLU activation function, and the second dense layer has 10 neurons with a softmax activation function, which produces a probability distribution over the 10 classes in the CIFAR-10 dataset.\n",
    "\n",
    "Overall, this model consists of three sets of convolutional and max pooling layers, followed by two dense layers, and is designed for image classification tasks on the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the model\n",
    "\n",
    "### The ADAM Optimizer\n",
    "The Adam optimizer is a popular optimization algorithm used in deep learning. It is a variant of stochastic gradient descent (SGD) that combines ideas from both Adagrad and RMSProp optimizers.\n",
    "\n",
    "The key idea behind Adam is to use adaptive learning rates for each weight in the neural network, which helps the optimizer converge faster and more reliably. It does this by keeping track of two exponential moving averages of the gradients and the gradient squared.\n",
    "\n",
    "The Adam optimizer uses the following update rule:\n",
    "\n",
    "Calculate the gradient of the loss function with respect to each weight in the neural network.\n",
    "Calculate the first and second moments of the gradient using exponential moving averages.\n",
    "Update the weights based on the first and second moments of the gradient and the learning rate.\n",
    "The learning rate in Adam is adaptive, which means it changes based on the first and second moments of the gradient. This helps the optimizer converge faster and more reliably compared to traditional SGD.\n",
    "\n",
    "Overall, the Adam optimizer is a powerful and widely used optimization algorithm in deep learning due to its ability to converge quickly and reliably on a wide range of tasks.\n",
    "\n",
    "### The Sparse Categorigal Cross-Entropy loss function\n",
    "\n",
    "Sparse categorical cross entropy is a loss function used in multi-class classification tasks where the target variable is represented by integers instead of one-hot encoded vectors. It compares the predicted probability distribution to the integer representation of the true label.\n",
    "\n",
    "In contrast to categorical cross-entropy, where the target variable is one-hot encoded, sparse categorical cross-entropy saves memory and computation time by only requiring a single integer value per target instead of a vector of binary values.\n",
    "\n",
    "Sparse categorical cross-entropy is commonly used in neural network models with a softmax output layer for multi-class classification problems where the number of classes is greater than two.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Tensorboard\n",
    "\n",
    "TensorBoard is a web-based visualization tool provided with TensorFlow. It allows users to monitor and debug machine learning models during the training process. TensorBoard provides a suite of visualization tools to help users understand, debug, and optimize their models.\n",
    "\n",
    "Some of the key features of TensorBoard include:\n",
    "\n",
    "Visualization of scalar values, such as loss and accuracy, as well as histograms of weight and bias distributions.\n",
    "Visualization of the computational graph of the model, including the input and output layers, as well as intermediate layers.\n",
    "Visualization of embeddings, which can be useful for visualizing high-dimensional data.\n",
    "Visualization of images, audio, and text data.\n",
    "Overall, TensorBoard helps users to understand the behavior of their models during training, and to identify potential issues such as overfitting or vanishing gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs\", histogram_freq=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test), callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print('Test loss:', test_loss)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on a single image from the test set\n",
    "test_image = x_test[0]\n",
    "test_image = test_image.reshape((1, 32, 32, 3))  # Reshape to match input shape of the model\n",
    "predictions = model.predict(test_image)\n",
    "predicted_class = np.argmax(predictions)\n",
    "\n",
    "# Print the predicted class label and the corresponding image\n",
    "print('Predicted class:', class_names[predicted_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x_test[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(class_names[y_test[i][0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
